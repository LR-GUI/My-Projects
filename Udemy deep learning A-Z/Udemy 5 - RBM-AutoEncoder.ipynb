{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use a RBM (restricted boltzmann machine) to recommend movies to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "movies = pd.read_csv('/home/gui/Downloads/udemy_deep_learning AZ/P16-Boltzmann-Machines/Boltzmann_Machines/ml-1m/movies.dat',\n",
    "                sep='::', header=None, engine='python', encoding = 'latin-1')\n",
    "\n",
    "users = pd.read_csv('/home/gui/Downloads/udemy_deep_learning AZ/P16-Boltzmann-Machines/Boltzmann_Machines/ml-1m/users.dat',\n",
    "                sep='::', header=None, engine='python', encoding = 'latin-1')\n",
    "\n",
    "ratings = pd.read_csv('/home/gui/Downloads/udemy_deep_learning AZ/P16-Boltzmann-Machines/Boltzmann_Machines/ml-1m/ratings.dat',\n",
    "                sep='::', header=None, engine='python', encoding = 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                   1                             2\n",
       "0  1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1  2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2  3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3  4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4  5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head() # Columns: MovieID, Title, Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2   3      4\n",
       "0  1  F   1  10  48067\n",
       "1  2  M  56  16  70072\n",
       "2  3  M  25  15  55117\n",
       "3  4  M  45   7  02460\n",
       "4  5  M  25  20  55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head() # Columns: UserID, Sex, Age, JobID, Zip Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1  2          3\n",
       "0  1  1193  5  978300760\n",
       "1  1   661  3  978302109\n",
       "2  1   914  3  978301968\n",
       "3  1  3408  4  978300275\n",
       "4  1  2355  5  978824291"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head() # Columns: UserId, MovieID, Rating, Time Stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3883, 3), (6040, 5), (1000209, 4))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.shape, users.shape, ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and test data are subsets of the ratings data\n",
    "# Training and test contain the same users rating different movies in each set\n",
    "\n",
    "ratings_train = pd.read_csv('/home/gui/Downloads/udemy_deep_learning AZ/P16-Boltzmann-Machines/Boltzmann_Machines/ml-100k/u1.base', delimiter='\\t')\n",
    "ratings_train = np.array(ratings_train, dtype='int')\n",
    "\n",
    "ratings_test = pd.read_csv('/home/gui/Downloads/udemy_deep_learning AZ/P16-Boltzmann-Machines/Boltzmann_Machines/ml-100k/u1.test', delimiter='\\t')\n",
    "ratings_test = np.array(ratings_test, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((79999, 4), (19999, 4))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_train.shape, ratings_test.shape # 80% / 20% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the number of users and movies in order to build rating matrices indexed by users and movies\n",
    "# That's because in our RBM we will use user ratings as observations and movies as features\n",
    "# See that IDs are sequential, so their max is the number of users/movies considered\n",
    "\n",
    "n_users = int(max(max(ratings_train[:,0]),max(ratings_test[:,0])))\n",
    "n_movies = int(max(max(ratings_train[:,1]),max(ratings_test[:,1])))\n",
    "\n",
    "n_users, n_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, format the train/test data in order to match pytorch input specifications\n",
    "# First, we want a \"matrix\" that is a list of lists, each row is a list containing the ratings of a user for each movie\n",
    "# If the user didn't rate a movie, the rating is 0 (ratings are integers from 1 to 5)\n",
    "\n",
    "def FormatData(data):\n",
    "    FData = []\n",
    "    for userID in range(1, n_users+1): # Loop for each user; userID starts at 1\n",
    "        movieID = data[:,1][data[:,0]==userID] # Take movieID for each movie rated by the user\n",
    "        user_ratings = data[:,2][data[:,0]==userID] # Now take the ratings\n",
    "        Fratings = np.zeros(n_movies) # Initialize new ratings with zeros to fill with rated movies\n",
    "        Fratings[movieID-1] = user_ratings # Fill with user ratings; movieID starts at 1 (broadcasting only on indexes given by movieID)\n",
    "        FData.append(list(Fratings))\n",
    "    return FData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fratings_train = FormatData(ratings_train)\n",
    "len(Fratings_train), len(Fratings_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fratings_test = FormatData(ratings_test)\n",
    "len(Fratings_test), len(Fratings_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 3., 4.,  ..., 0., 0., 0.],\n",
       "        [4., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [5., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 5., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, finalize data transformation by turning this matrix into a pytorch tensor\n",
    "# A tensor is just a multidimensional matrix that could be implemented with numpy\n",
    "# We use pytorch for computational efficiency (numpy is not optimized for tensors)\n",
    "\n",
    "train = torch.FloatTensor(Fratings_train) # There will be used by the RBM\n",
    "test = torch.FloatTensor(Fratings_test)\n",
    "\n",
    "AE_train = torch.FloatTensor(Fratings_train) # Take train and test for the auto encoder\n",
    "AE_test = torch.FloatTensor(Fratings_test)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBM\n",
    "\n",
    "# With the boltzmann machine, we want to know whether a user will like or not a new movie\n",
    "# To make our input match, we make 0 rating = -1; 1, 2, and 3 rating = 0 (not liked); 4 and 5 rating = 1 (liked)\n",
    "\n",
    "train[train==0] = -1\n",
    "train[train==1] = 0\n",
    "train[train==2] = 0\n",
    "train[train==3] = 1\n",
    "train[train>=4] = 1\n",
    "\n",
    "test[test==0] = -1\n",
    "test[test==1] = 0\n",
    "test[test==2] = 0\n",
    "test[test==3] = 1\n",
    "test[test>=4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1682]), torch.Size([100, 1682]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].shape, train[:100].shape # dim(n_movies), dim(batch_size, n_movies); movies = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RBM class\n",
    "\n",
    "# What the RBM will do? Users in train and test sets are the same, but the train set says the user watched some movies\n",
    "# and the test set says the user watched some all different movies. So, the RBM takes the train set and learns the\n",
    "# weights so as to function as a posterior density sampler that correctly correlates similar movies, based on how users\n",
    "# liked movies together. Then, a new data point inputs with some watched movies and receives a sample given by the\n",
    "# trained RBM with movies they will like and movies they will not like, based on the submitted ratings. For testing,\n",
    "# we will take an user from the training set and input it to the RMB. We receive the recommendations for the remainning\n",
    "# movies and then compare with the ratings that the same user gives in the test set. If the RBM is good, its\n",
    "# recommendations will match the new movies rated by that user in the test set. For a prediction, you just input your\n",
    "# ratings and receive the recommendation sampled by the RBM, saying that you'll like some of the other movies and won't\n",
    "# like the rest\n",
    "\n",
    "class RBM():\n",
    "    def __init__(self, nv, nh): # Number of hidden and visible nodes\n",
    "        self.W = torch.randn(nh, nv) # RMB's connection weights between hidden and visible nodes, starts randomly\n",
    "        self.a = torch.randn(1, nh) # Bias for prob. of hidden nodes iven visible, 1d form is made compatible to 2d\n",
    "        self.b = torch.randn(1, nv) # Bias for prob. of visible nodes given hidden\n",
    "    \n",
    "    # In the RBM, each activation function is a conditional probability that is given by a sigmoid function\n",
    "    # The weights are adjusted by minimizing a log likelihood function\n",
    "    # The log-likelihood gradients are approximated by contrastive divergence, that uses k-fold Gibbs sampling to\n",
    "    # sample from visible nodes to update hidden samples and sample from hidden nodes to update visible samples\n",
    "    # At the end of k Gibbs iterations, weights are updated with gradients approximated by the contrastive divergence\n",
    "    \n",
    "    def sample_h(self, x): # x will be a tensor of visible nodes of dim(batch_size, nv)\n",
    "        xw = torch.mm(x, self.W.t()) # (batch_size, nv) x (nv, nh) = (batch_size, nh)\n",
    "        activation = xw + self.a.expand_as(xw) # we expand a from (1, nh) to (batch_size, nh)\n",
    "        ph_given_v = torch.sigmoid(activation)\n",
    "        return ph_given_v, torch.bernoulli(ph_given_v) # h sample and ph, of dim (batch_size, nh)\n",
    "    \n",
    "    def sample_v(self, y): # the same as sample_h, exchanging v and h\n",
    "        yw = torch.mm(y, self.W)\n",
    "        activation = yw + self.b.expand_as(yw)\n",
    "        pv_given_h = torch.sigmoid(activation)\n",
    "        return pv_given_h, torch.bernoulli(pv_given_h)\n",
    "    \n",
    "    def train(self, vo, pho, vk, phk): # train by contrastive divergence, using input data against k-fold Gibbs samples\n",
    "        self.W += (torch.mm(vo.t(),pho) - torch.mm(vk.t(),phk)).t()\n",
    "        self.a += torch.sum((pho-phk),0) # torch.sum aggregates tensors, dim(x) is compatible with dim(1, x)\n",
    "        self.b += torch.sum((vo-vk),0)\n",
    "        \n",
    "    def predict(self, new_data):\n",
    "        _, h = self.sample_h(new_data)\n",
    "        _, v = self.sample_v(h)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize RMB\n",
    "\n",
    "nv = len(train[0]) # number of visible nodes, number of features\n",
    "nh = 100 # number of hidden nodes\n",
    "batch_size = 100 # how many users go into each training step\n",
    "\n",
    "rbm = RBM(nv,nh)\n",
    "\n",
    "nv, nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss:  tensor(0.3419)\n",
      "epoch:  2  loss:  tensor(0.2461)\n",
      "epoch:  3  loss:  tensor(0.2481)\n",
      "epoch:  4  loss:  tensor(0.2506)\n",
      "epoch:  5  loss:  tensor(0.2474)\n",
      "epoch:  6  loss:  tensor(0.2452)\n",
      "epoch:  7  loss:  tensor(0.2488)\n",
      "epoch:  8  loss:  tensor(0.2473)\n",
      "epoch:  9  loss:  tensor(0.2448)\n",
      "epoch:  10  loss:  tensor(0.2477)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_epochs = 10 # number of training steps\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = 0 # loss function to assess error between epochs; measures difference between k-th sample and the input data\n",
    "    s = 0. # counts the number of epochs for loss normalization\n",
    "    \n",
    "    for userID in range(0, n_users - batch_size, batch_size): # initialize batch for training, \n",
    "        vk = train[userID:userID+batch_size]\n",
    "        vo = train[userID:userID+batch_size]\n",
    "        \n",
    "        for k in range(10): # k-fold Gibbs sampling, hidden nodes update visible nodes's density and vice versa\n",
    "            _, hk = rbm.sample_h(vk)\n",
    "            _, vk = rbm.sample_v(hk)\n",
    "            vk[vo<0] = vo[vo<0] # reset unwatched movies having meaningless update\n",
    "        \n",
    "        pho, _ = rbm.sample_h(vo) # take hidden density values\n",
    "        phk, _ = rbm.sample_h(vk)\n",
    "        \n",
    "        rbm.train(vo, pho, vk, phk) # train to update weights based on obtained vk and phk\n",
    "        \n",
    "        loss += torch.mean(torch.abs(vo[vo>=0]-vk[vo>=0])) # loss func is mean absolute difference (it's 0 or 1 anyway)\n",
    "        s += 1.\n",
    "    print('epoch: ', str(epoch), ' loss: ', str(loss/s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1682]), torch.Size([1682]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[2:3].shape, train[2].shape # see the difference in dimensions, we will use this below... this is because the first\n",
    "                                 # is still a slice of train and the second is an element of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: tensor(0.2512)\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for userID in range(n_users):\n",
    "    v = train[userID:userID+1] # Take the user's training ratings\n",
    "    vt = test[userID:userID+1] # Take the user's test ratings\n",
    "    \n",
    "    if len(vt[vt>=0]) > 0: # To ensure that we really have that user rating movies in the training set\n",
    "        _,h = rbm.sample_h(v) # Sample hidden nodes that will give the posterior for recommendations\n",
    "        _,v = rbm.sample_v(h) # Generate recommendations\n",
    "        test_loss += torch.mean(torch.abs(vt[vt>=0] - v[vt>=0])) # Loss compares recommendations with test ratings\n",
    "        s += 1.\n",
    "\n",
    "print('test loss: '+str(test_loss/s)) # Test loss is good if kept close to train loss (meaning no overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test user 21 watched 84 movies\n",
      "And liked 37 of them\n",
      "Number of recommended movies: 1160\n",
      "Number of not recommended movies: 522\n"
     ]
    }
   ],
   "source": [
    "# A prediction for user 21\n",
    "\n",
    "pred = rbm.predict(test[20:21])\n",
    "print('Test user 21 watched', str(len(test[20:21][test[20:21]>=0])), 'movies')\n",
    "print('And liked', len(test[20:21][test[20:21]==0]) , 'of them')\n",
    "print('Number of recommended movies:', str(len(pred[pred==1])))\n",
    "print('Number of not recommended movies:', str(n_movies - len(pred[pred==1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True, False,  True,\n",
       "         True,  True, False, False,  True, False,  True, False,  True,  True,\n",
       "         True,  True, False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that the user receives recommendations for already watched movies, and some of them are wrong\n",
    "test[20:21][test[20:21]>0] == pred[test[20:21]>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we build an AutoEncoder to give non-binary recommendations, the actual predicted ratings, from 1 to 5\n",
    "\n",
    "# Autoencoders are neural networks for unsupervised learning. It is a method of data compression / dimensionality\n",
    "# reduction. Take an input layer and an output layer of the same size. Now, add a smaller hidden layer. The algorithm\n",
    "# tries to learn the identity function, making the output the same as the input. If the input data is random, without\n",
    "# structure, this will be hard. If it's structured, the network will be able to reproduce the input by learning the\n",
    "# structure with the hidden layer. Then, the hidden layer will be a smaller representation of the input data, with\n",
    "# reduced features / dimensions. We can also make the hidden layer bigger an add a sparcity constraint to make the\n",
    "# neurons fire way less on average; this makes it possible for data structure to be learned more efficiently at the cost\n",
    "# of dimensionality reduction. This is because we can associate in a more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked AutoEncoder class\n",
    "# This architecture is a kind of deep autoencoder, with 3 hidden layers, of 20, 10, and 20 nodes\n",
    "\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__() # Necessary for pytorch nn classes\n",
    "        self.fc1 = nn.Linear(n_movies, 20) # This function builds the architecture\n",
    "        self.fc2 = nn.Linear(20,10)\n",
    "        self.fc3 = nn.Linear(10,20)\n",
    "        self.fc4 = nn.Linear(20, n_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x): # This function passes the data through the network\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "\n",
    "sae = SAE()\n",
    "\n",
    "loss = nn.MSELoss() # Mean Square Error loss function\n",
    "optimi = optim.RMSprop(sae.parameters(), lr=0.01, weight_decay=0.5) # We choose RMSprop as optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1  loss : 1.7715149109148893\n",
      "epoch : 2  loss : 1.096687728815192\n",
      "epoch : 3  loss : 1.0534122318199772\n",
      "epoch : 4  loss : 1.0383853239892509\n",
      "epoch : 5  loss : 1.030818054192946\n",
      "epoch : 6  loss : 1.0265354084312588\n",
      "epoch : 7  loss : 1.0238244704641435\n",
      "epoch : 8  loss : 1.0220064819749113\n",
      "epoch : 9  loss : 1.020216112204642\n",
      "epoch : 10  loss : 1.0197007850983588\n",
      "epoch : 11  loss : 1.0185181032649502\n",
      "epoch : 12  loss : 1.0184571359518915\n",
      "epoch : 13  loss : 1.0179434414347033\n",
      "epoch : 14  loss : 1.017452886892715\n",
      "epoch : 15  loss : 1.017495548274439\n",
      "epoch : 16  loss : 1.0169047779410412\n",
      "epoch : 17  loss : 1.0165627230521688\n",
      "epoch : 18  loss : 1.016262887091639\n",
      "epoch : 19  loss : 1.0161386072587186\n",
      "epoch : 20  loss : 1.0160162400660269\n",
      "epoch : 21  loss : 1.0159327388513368\n",
      "epoch : 22  loss : 1.0161472004241539\n",
      "epoch : 23  loss : 1.0157173420009935\n",
      "epoch : 24  loss : 1.0158304517535013\n",
      "epoch : 25  loss : 1.015597894480981\n",
      "epoch : 26  loss : 1.0153955775904588\n",
      "epoch : 27  loss : 1.0152723895530484\n",
      "epoch : 28  loss : 1.0150535550628583\n",
      "epoch : 29  loss : 1.0126653277749844\n",
      "epoch : 30  loss : 1.0115157539697708\n",
      "epoch : 31  loss : 1.0094521132500358\n",
      "epoch : 32  loss : 1.0092825908069076\n",
      "epoch : 33  loss : 1.005347638585594\n",
      "epoch : 34  loss : 1.0047385066770402\n",
      "epoch : 35  loss : 1.0012802223318238\n",
      "epoch : 36  loss : 1.0006083988552532\n",
      "epoch : 37  loss : 0.9977672872649054\n",
      "epoch : 38  loss : 0.995798244853869\n",
      "epoch : 39  loss : 0.9927406679904476\n",
      "epoch : 40  loss : 0.9931513668726901\n",
      "epoch : 41  loss : 0.9900287403518702\n",
      "epoch : 42  loss : 0.988176635031295\n",
      "epoch : 43  loss : 0.9853143903585653\n",
      "epoch : 44  loss : 0.9859755319092275\n",
      "epoch : 45  loss : 0.9803940697131835\n",
      "epoch : 46  loss : 0.9797183596330993\n",
      "epoch : 47  loss : 0.9782455593098582\n",
      "epoch : 48  loss : 0.9755666570831079\n",
      "epoch : 49  loss : 0.9731538937966815\n",
      "epoch : 50  loss : 0.9762996697013715\n",
      "epoch : 51  loss : 0.9716410101258276\n",
      "epoch : 52  loss : 0.9692167441112995\n",
      "epoch : 53  loss : 0.9668485488872518\n",
      "epoch : 54  loss : 0.9661918928082971\n",
      "epoch : 55  loss : 0.9620113103700008\n",
      "epoch : 56  loss : 0.9594210102770263\n",
      "epoch : 57  loss : 0.959388622456254\n",
      "epoch : 58  loss : 0.9705222032099583\n",
      "epoch : 59  loss : 0.9698874440982885\n",
      "epoch : 60  loss : 0.9704080853858021\n",
      "epoch : 61  loss : 0.9684599873200976\n",
      "epoch : 62  loss : 0.9669692493542472\n",
      "epoch : 63  loss : 0.9612758193528484\n",
      "epoch : 64  loss : 0.9641456662963624\n",
      "epoch : 65  loss : 0.9611277187497652\n",
      "epoch : 66  loss : 0.9617356765235903\n",
      "epoch : 67  loss : 0.959060083259066\n",
      "epoch : 68  loss : 0.960695351410489\n",
      "epoch : 69  loss : 0.9578879192780053\n",
      "epoch : 70  loss : 0.9552432995797328\n",
      "epoch : 71  loss : 0.9551819268412485\n",
      "epoch : 72  loss : 0.9541603754143718\n",
      "epoch : 73  loss : 0.9543100392429736\n",
      "epoch : 74  loss : 0.9567888540513695\n",
      "epoch : 75  loss : 0.9573784072239911\n",
      "epoch : 76  loss : 0.9547352214035072\n",
      "epoch : 77  loss : 0.9573324814441786\n",
      "epoch : 78  loss : 0.9633804035596047\n",
      "epoch : 79  loss : 0.9681222024984759\n",
      "epoch : 80  loss : 0.9693574901194358\n",
      "epoch : 81  loss : 0.9645319495806176\n",
      "epoch : 82  loss : 0.9637136559782885\n",
      "epoch : 83  loss : 0.9579236181009941\n",
      "epoch : 84  loss : 0.9563964929260029\n",
      "epoch : 85  loss : 0.9562237206592544\n",
      "epoch : 86  loss : 0.9526474089123826\n",
      "epoch : 87  loss : 0.9486630987132326\n",
      "epoch : 88  loss : 0.9527798054385748\n",
      "epoch : 89  loss : 0.9478009142176458\n",
      "epoch : 90  loss : 0.9496327552533039\n",
      "epoch : 91  loss : 0.9498889709749173\n",
      "epoch : 92  loss : 0.9490389797961488\n",
      "epoch : 93  loss : 0.9451582823645043\n",
      "epoch : 94  loss : 0.946645760040913\n",
      "epoch : 95  loss : 0.9439887709618804\n",
      "epoch : 96  loss : 0.9559801023864326\n",
      "epoch : 97  loss : 0.9521151760528256\n",
      "epoch : 98  loss : 0.949687641934546\n",
      "epoch : 99  loss : 0.9468005666933237\n",
      "epoch : 100  loss : 0.947170786974216\n",
      "epoch : 101  loss : 0.945737073318488\n",
      "epoch : 102  loss : 0.9485106805248352\n",
      "epoch : 103  loss : 0.946498421222582\n",
      "epoch : 104  loss : 0.9452314229572742\n",
      "epoch : 105  loss : 0.9418731534518462\n",
      "epoch : 106  loss : 0.9427887740704923\n",
      "epoch : 107  loss : 0.9387775989872079\n",
      "epoch : 108  loss : 0.9405974864248632\n",
      "epoch : 109  loss : 0.9490722788273794\n",
      "epoch : 110  loss : 0.9503683911661535\n",
      "epoch : 111  loss : 0.9496128753550684\n",
      "epoch : 112  loss : 0.9485290948908758\n",
      "epoch : 113  loss : 0.9464335969312422\n",
      "epoch : 114  loss : 0.9477573641255176\n",
      "epoch : 115  loss : 0.9455698449886316\n",
      "epoch : 116  loss : 0.9459626643385385\n",
      "epoch : 117  loss : 0.944402209400436\n",
      "epoch : 118  loss : 0.9440675323186251\n",
      "epoch : 119  loss : 0.9432415466091613\n",
      "epoch : 120  loss : 0.9428460752852585\n",
      "epoch : 121  loss : 0.9410255722036565\n",
      "epoch : 122  loss : 0.9417887964173725\n",
      "epoch : 123  loss : 0.9403169494175089\n",
      "epoch : 124  loss : 0.9415984210909665\n",
      "epoch : 125  loss : 0.9401636981065306\n",
      "epoch : 126  loss : 0.9395290949333978\n",
      "epoch : 127  loss : 0.9383183934952462\n",
      "epoch : 128  loss : 0.9399047207716659\n",
      "epoch : 129  loss : 0.9387714528225141\n",
      "epoch : 130  loss : 0.9386796790889049\n",
      "epoch : 131  loss : 0.9381215782141614\n",
      "epoch : 132  loss : 0.9409758663394766\n",
      "epoch : 133  loss : 0.9385359405414593\n",
      "epoch : 134  loss : 0.9389342862733077\n",
      "epoch : 135  loss : 0.9369572102360153\n",
      "epoch : 136  loss : 0.9383276661663812\n",
      "epoch : 137  loss : 0.9368438420760763\n",
      "epoch : 138  loss : 0.9373877093390474\n",
      "epoch : 139  loss : 0.9355697004512847\n",
      "epoch : 140  loss : 0.9364775250899101\n",
      "epoch : 141  loss : 0.934704091536924\n",
      "epoch : 142  loss : 0.9352843825043642\n",
      "epoch : 143  loss : 0.9360572232186125\n",
      "epoch : 144  loss : 0.9353422485968947\n",
      "epoch : 145  loss : 0.9329463882915978\n",
      "epoch : 146  loss : 0.9337440415654484\n",
      "epoch : 147  loss : 0.9325684177535903\n",
      "epoch : 148  loss : 0.9337182423833525\n",
      "epoch : 149  loss : 0.9311747161943491\n",
      "epoch : 150  loss : 0.9320781055728431\n",
      "epoch : 151  loss : 0.931092622320408\n",
      "epoch : 152  loss : 0.9317684938187183\n",
      "epoch : 153  loss : 0.9308555577553459\n",
      "epoch : 154  loss : 0.931493255226785\n",
      "epoch : 155  loss : 0.9301192061461001\n",
      "epoch : 156  loss : 0.9306168910766907\n",
      "epoch : 157  loss : 0.9295329613386921\n",
      "epoch : 158  loss : 0.9303192434251782\n",
      "epoch : 159  loss : 0.9290859161632735\n",
      "epoch : 160  loss : 0.9298344416113417\n",
      "epoch : 161  loss : 0.9285834297327368\n",
      "epoch : 162  loss : 0.9291029391287559\n",
      "epoch : 163  loss : 0.9283029727705471\n",
      "epoch : 164  loss : 0.9333657630876093\n",
      "epoch : 165  loss : 0.9312422247847228\n",
      "epoch : 166  loss : 0.928903441097837\n",
      "epoch : 167  loss : 0.928663296930887\n",
      "epoch : 168  loss : 0.9278533930584981\n",
      "epoch : 169  loss : 0.926701459505024\n",
      "epoch : 170  loss : 0.9271509101942274\n",
      "epoch : 171  loss : 0.9262045652534665\n",
      "epoch : 172  loss : 0.9268046673608595\n",
      "epoch : 173  loss : 0.9256054571977234\n",
      "epoch : 174  loss : 0.9262513274442471\n",
      "epoch : 175  loss : 0.9253156050617983\n",
      "epoch : 176  loss : 0.925641806951727\n",
      "epoch : 177  loss : 0.9249280008402354\n",
      "epoch : 178  loss : 0.9253635318172477\n",
      "epoch : 179  loss : 0.9241379832660217\n",
      "epoch : 180  loss : 0.9250152020296533\n",
      "epoch : 181  loss : 0.9243362174568736\n",
      "epoch : 182  loss : 0.9244481294757086\n",
      "epoch : 183  loss : 0.9236609082313154\n",
      "epoch : 184  loss : 0.9237328488221048\n",
      "epoch : 185  loss : 0.9230051186381577\n",
      "epoch : 186  loss : 0.9234033891896425\n",
      "epoch : 187  loss : 0.922724594872271\n",
      "epoch : 188  loss : 0.9229112220420704\n",
      "epoch : 189  loss : 0.9224904634709374\n",
      "epoch : 190  loss : 0.9225838157721568\n",
      "epoch : 191  loss : 0.9219918306989738\n",
      "epoch : 192  loss : 0.9220919690415428\n",
      "epoch : 193  loss : 0.9218260548408814\n",
      "epoch : 194  loss : 0.9219121439863611\n",
      "epoch : 195  loss : 0.9209844052425935\n",
      "epoch : 196  loss : 0.9214451836863428\n",
      "epoch : 197  loss : 0.9209088769146813\n",
      "epoch : 198  loss : 0.9211320680844012\n",
      "epoch : 199  loss : 0.9208454593031963\n",
      "epoch : 200  loss : 0.9211800265537449\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "n_epochs = 200\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    \n",
    "    for userID in range(n_users):\n",
    "        D = AE_train[None, userID] # Take data\n",
    "        T = D.clone() # Take target, here is the same, AE compares the output with the input itself\n",
    "        \n",
    "        if torch.sum(T.data > 0) > 0: # User has to have rated a movie\n",
    "            output = sae(D)\n",
    "            T.require_grad = False \n",
    "            output[T==0] = 0 # Reset non-rated movies\n",
    "            L = loss(output,T)\n",
    "            mean_corrector = n_movies/float(torch.sum(T.data > 0) + 1e-10) # Leave out non watched movies\n",
    "            L.backward() # Compute loss\n",
    "            train_loss += np.sqrt(L.item()*mean_corrector)\n",
    "            s += 1.\n",
    "            optimi.step() # Compute the optimization\n",
    "    print('epoch :', epoch, ' loss :', train_loss/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss : 0.9712291417439184\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "\n",
    "for userID in range(n_users):\n",
    "    D = AE_train[None, userID] # Take data\n",
    "    T = AE_test[None, userID] # Take target, here it compares the output with the test set\n",
    "    \n",
    "    if torch.sum(T.data > 0) > 0:\n",
    "        output = sae(D)\n",
    "        T.require_grad = False\n",
    "        output[T==0] = 0\n",
    "        L = loss(output,T)\n",
    "        mean_corrector = n_movies/float(torch.sum(T.data > 0) + 1e-10)\n",
    "        L.backward()\n",
    "        test_loss += np.sqrt(L.item()*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss :', test_loss/s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, on average, our autoencoder is off by slightly less than 1 star in reccommending actual ratings, from 1 to 5 stars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
